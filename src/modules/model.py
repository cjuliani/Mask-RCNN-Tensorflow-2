import os
import tensorflow as tf
import config

from src.modules.utils.module import get_module
from src.modules.networks import rpn, roi_head, mask_head
from src.modules.utils.rcnn import anchor_labels, anchor_targets, roi_ops, anchor_generate, bbox_ops
from src.modules.utils import learning_rate, nms


class Model(tf.keras.Model):
    """The learning model consisting of method for train step,
    optimization, prediction, loss and evaluation metrics.

    Attributes:
        num_anchors (int): the number of box anchors generated per input.
        num_classes (int): then number of classes.
        image_width (int): the width of input images.
        image_height (int): the height of input images.
        batch_size (int): the size of individual batch.
        class_names: the list of unique class names.
        class_values: the list of unique class IDs.
        anchors (float): the bounding box anchors.
        gt_anchor_assignment (int): the training box IDs associated with the
            predicted ones.
        rpn_assignment_ratio (float): the ratio of training boxes generated
            by the RPN spatially overlapping with the predicted ones.
        bg_obj_indices (float): indices of boxes associated to the background
            in the batch.
        nms_bbox (float): coordinates of the bounding boxes kept from the
            non-maximum suppression module.
        nms_scores (float): object scores of the bounding boxes kept from
            the non-maximum suppression module.
        head_bbox_from_rpn (float): bounding boxes generated by the RPN and
            used for training the HEAD network.
        head_scores_from_rpn (float): object scores generated by the RPN and
            used for training the HEAD network.
        head_gt_classes (float): ground truth classes of training boxes from
            the batch input to the HEAD network.
        head_bbox_targets (float): ground truth bounding box coordinates as
            batch input to the HEAD network.
        head_bbox_weights (float): weights of bounding box coordinates as
            batch input to the HEAD network.
        head_cls_scores (float): class scores of bounding boxes as inputs to
            the HEAD network.
        head_cls (float): predicted object classes from the HEAD network.
        head_bbox_enc (float): encoded bounding boxes from the HEAD network.
        head_assignment_ratio (float): the ratio of training boxes generated
            by the HEAD spatially overlapping with the predicted ones.
        head_ious (float): IoU values of proposals estimated from the HEAD
            network results.
        head_cls_prob (float): probability scores of object classes estimated
            from the HEAD network.
        head_kept_indices (float): indices of bounding boxes considered for the
            HEAD network.
        anchor_pos_neg_scores (float): scores of bounding box anchors.
        kept_indices (float): indices of bounding boxes considered for the
            RPN network.
        rpn_anchor_proposals (float): coordinates of anchor proposals from the
            RPN network.
        rpn_bbox_targets (float): target values of bounding boxes as reference data
            to the RPN network.
        rpn_proposals (float): coordinates of box proposals from the RPN network.
        rpn_gt_assignments (float): the training box IDs associated with the
            predicted ones from the RPN network.
        rpn_bbox_enc (float): encoded bounding boxes from the RPN network.
        rpn_anchor_bbox (float): the bounding box anchors from the RPN network.
        rpn_pos_neg_scores (float): scores of bounding boxes from the RPN network.
        rpn_scores_enc (float): predicted object classes from the RPN network.
        rpn_anchor_scores_prob (float): probability scores of object classes estimated
            from the RPN network.
        rpn_anchor_scores (float): the object scores of bounding boxes estimated from
            the RPN network.
        rois_pooled (float): encoded bounding boxes from the RPN network after applying
            the region of interest alignment (RoI Align).
        rpn_bbox_enc (float): encoded bounding boxes from the RPN network.
        rpn (object): the region proposal network (RPN).
        bbox_head (object): the HEAD network.
        networks (list): list of learning network objects available in this module (backbone,
            RPN and HEAD).
        learning_rate: the learning rate used in training.
        optimizer (keras.optimizer_v2.gradient_descent.SGD): the optimization module to update
            the model parameters.
    """
    def __init__(self, num_classes, input_size, learning_prop,
                 batch_size, metric_interval, adam_optimizer=False, **kwargs):
        super(Model, self).__init__(**kwargs)

        self.num_anchors = tf.constant(
            len(config.ANCHOR_SCALES) * len(config.ANCHOR_RATIOS))
        self.num_classes = num_classes
        self.image_width = input_size[0]
        self.image_height = input_size[1]
        self.batch_size = batch_size
        self.class_names = tf.Variable(
            [], dtype=tf.string, trainable=False, shape=tf.TensorShape(None))
        self.class_values = tf.Variable(
            [], dtype=tf.int32, trainable=False, shape=tf.TensorShape(None))
        self.anchors = tf.zeros((0, 4))
        self.gt_anchor_assignment = tf.zeros((0, 1), dtype=tf.int32)
        self.rpn_assignment_ratio = tf.zeros((0, 1), dtype=tf.float32)
        self.bg_obj_indices = tf.zeros((0, 1))
        self.nms_bbox = tf.zeros((0, 0, 4))
        self.nms_scores = tf.zeros((0, 0, 1))
        self.head_bbox_from_rpn = tf.zeros((0, 0, 4))
        self.head_scores_from_rpn = tf.zeros((0, 0, 1))
        self.head_gt_classes = tf.zeros((0, 1), dtype=tf.int32)
        self.head_gt_masks = tf.zeros((0, 0, self.image_height, self.image_width), dtype=tf.int32)
        self.head_bbox_targets = tf.zeros((0, 0, 4 * num_classes))
        self.head_bbox_weights = tf.zeros((0, 0, 4 * num_classes))
        self.head_cls_scores = tf.zeros((0, 1, num_classes))
        self.head_cls = tf.zeros((0, 1))
        self.head_bbox_enc = tf.zeros((0, 0, 4 * num_classes))
        self.head_assignment_ratio = tf.zeros((0, 1))
        self.head_ious = tf.zeros((0, 1))
        self.head_cls_prob = tf.zeros((0, 1))
        self.head_kept_indices = tf.zeros((0, 0), dtype=tf.int32)
        self.anchor_pos_neg_scores = tf.zeros((0, 1))
        self.kept_indices = tf.zeros((0, 0), dtype=tf.int32)
        self.rpn_anchor_proposals = tf.zeros((0, 0, 4))
        self.rpn_bbox_targets = tf.zeros((0, 0, 4))
        self.rpn_proposals = tf.zeros((0, 0, 4))
        self.rpn_gt_assignments = tf.zeros((0, 1))
        self.rpn_bbox_enc = tf.zeros((0, 0, 4))
        self.rpn_anchor_bbox = tf.zeros((0, 0, 4))
        self.rpn_pos_neg_scores = tf.zeros((0, 1), dtype=tf.int32)
        self.rpn_scores_enc = tf.zeros((0, 0, 2))
        self.rpn_anchor_scores_prob = tf.zeros((0, 0, 2))
        self.rpn_anchor_scores = tf.zeros((0, 0, 2))
        self.rois_pooled = tf.zeros((0, 0, 0, 1))
        self.rois_pooled_masks_prob = tf.zeros((0, 0, 0, num_classes))
        self.rois_pooled_masks_logits = tf.zeros((0, 0, 0, num_classes))
        self.rois_pooled_mask = tf.zeros((0, 0, 0, 1))

        # Build backbone model.
        backbone_module = get_module(config.BACKBONE_PATH, config.BACKBONE_MODEL)

        self.backbone_model = backbone_module.Model(
            trainable=config.TRAIN_BACKBONE,
            add_initial_weights=config.LOAD_IMAGENET_WEIGHTS,
            layers_to_change=config.ANCHOR_DENSITY_LEVEL,
            reg_coef=config.BACKBONE_REG_COEF,
            weights_folder=config.PATH_IMAGENET_WEIGHTS)

        # Build region proposal network (RPN) and initialize
        # related outputs.
        self.rpn = rpn.RPN(
            num_anchor_types=self.num_anchors,
            image_width=tf.cast(self.image_width, tf.float32),
            image_height=tf.cast(self.image_height, tf.float32),
            trainable=config.TRAIN_RPN,
            reg_coef=config.RPN_REG_COEF,
            metric_interval=metric_interval)

        # Build bounding box prediction network.
        self.bbox_head = roi_head.RoiHead(
            num_classes,
            trainable=config.TRAIN_HEAD,
            metric_interval=metric_interval)

        self.bbox_masking = mask_head.MaskHead(
            num_classes,
            trainable=config.TRAIN_MASK,
            metric_interval=metric_interval)

        # Define list of nested networks
        self.networks = [self.backbone_model.model, self.rpn,
                         self.bbox_head, self.bbox_masking]

        if learning_prop is not None:
            self.learning_rate = tf.Variable(
                learning_prop.learning_rate, trainable=False, name="learning_rate")

            # Define learning rate with/without decay.
            if learning_prop.lr_decay:
                if learning_prop.lr_cyclic:
                    # Cyclic decay.
                    lr_schedule = learning_rate.CyclicLR(
                        max_lr=self.learning_rate,
                        base_lr=learning_prop.minimum_lr,
                        step_size=learning_prop.cyclic_step_size,
                        gamma=learning_prop.cyclic_gamma)
                else:
                    # Exponential decay.
                    lr_schedule = learning_rate.ExponentialDecay(
                        initial_lr=self.learning_rate,
                        decay_steps=learning_prop.decay_steps,
                        decay_rate=learning_prop.decay_rate,
                        staircase=learning_prop.decay_staircase,
                        minimum_lr=learning_prop.minimum_lr)
            else:
                # No decay.
                lr_schedule = learning_rate.NoDecay(
                    initial_lr=self.learning_rate)

            # Define optimizer.
            if not adam_optimizer:
                self.optimizer = tf.keras.optimizers.SGD(
                    learning_rate=lr_schedule,
                    momentum=learning_prop.momentum,
                    nesterov=learning_prop.nesterov)
            else:
                self.optimizer = tf.keras.optimizers.Adam(
                    learning_rate=lr_schedule)
        else:
            # Used in testing or predicting phase.
            self.learning_rate = tf.Variable(
                0., trainable=False, name="learning_rate")

            self.optimizer = tf.keras.optimizers.SGD(
                learning_rate=self.learning_rate,
                momentum=0.,
                nesterov=False)

    @staticmethod
    def extract_input_regions(inputs, proposals, image_size,
                              box_indices, crop_size):
        """Returns a region of img cropped at given box coordinates.

        Args:
            inputs (tensor): the image input to extract objects from.
            proposals (tensor): coordinates of the box proposals.
            image_size (tuple): the size of img images by side.
            box_indices (tensor): indices of box considered for cropping.
            crop_size (int): the cropping size defined per bounding box.
        """
        x0, y0, x1, y1 = tf.split(proposals, 4, axis=1)
        normalized_bbox = tf.concat(
            [y0 / tf.cast(image_size[1] - 1, dtype=tf.float32),
             x0 / tf.cast(image_size[0] - 1, dtype=tf.float32),
             y1 / tf.cast(image_size[1] - 1, dtype=tf.float32),
             x1 / tf.cast(image_size[0] - 1, dtype=tf.float32)], axis=1)

        return tf.image.crop_and_resize(
            image=inputs,
            boxes=normalized_bbox,
            box_indices=box_indices,
            crop_size=tf.constant([crop_size, crop_size]),
            method='bilinear',
            name="input_crops")

    @tf.function(input_signature=(
            tf.TensorSpec(shape=(None, None, None, 3), dtype=tf.float32),
            tf.TensorSpec(shape=(None, None, 4), dtype=tf.int32),
            tf.TensorSpec(shape=(None, None,), dtype=tf.int32),
            tf.TensorSpec(shape=(None, None, None, None), dtype=tf.int32),
            tf.TensorSpec(shape=None, dtype=tf.bool)))
    def call(self, inputs, gt_boxes=None, gt_cls=None, gt_masks=None, is_training=True):
        """Returns the losses and metrics given the input tensors.

        Args:
            inputs (tensor): the input tensors to the learning model.
            gt_boxes (tensor): ground truth box coordinates to learn from.
            gt_cls (tensor): reference data to learn from (labels).
            gt_masks (tensor): reference masks from generated data.
            is_training (bool): if True, the backbone model is trainable.
        """
        # Get networks outputs.
        self.run_networks(
            inputs, gt_boxes=gt_boxes, gt_cls=gt_cls, gt_masks=gt_masks,
            is_training=is_training)

        # Define rpn scores probabilities.
        rpn_scores_prob = tf.nn.softmax(self.rpn_scores_enc, axis=-1)  # (B, rpn_B, 2)

        # Get positive and unknown proposals from RPN.
        pos_rpn_indices = tf.where(
            tf.greater(self.rpn_pos_neg_scores, 0)  # (B, rpn_B)
        )  # (B*rpn_pos, 2), incl. rows & cols
        pos_rpn_indices = tf.reshape(pos_rpn_indices[:, 1], (self.batch_size, -1))  # (B, rpn_pos), only cols

        pos_rpn_proposals = tf.reshape(
            tf.gather(self.rpn_proposals, pos_rpn_indices, batch_dims=1), (-1, 4))  # (B*rpn_pos, 4)

        gt_boxes_assigned = tf.gather(gt_boxes, self.rpn_gt_assignments, batch_dims=1)  # (B, rpn_B, 4)
        pos_rpn_gt_bbox = tf.reshape(tf.gather(
            tf.cast(gt_boxes_assigned, tf.float32), pos_rpn_indices, batch_dims=1), (-1, 4))  # (B*rpn_pos, 4)

        # --------
        # Get head proposals.
        tiled_anchors = tf.tile(tf.expand_dims(self.anchors, axis=0), tf.constant([self.batch_size, 1, 1]))
        head_proposals = self.bbox_head.get_proposals(
            gt_cls=tf.reshape(self.head_gt_classes, [-1]),  # (B*head_B,)
            targets=tf.reshape(self.head_bbox_targets, [-1, self.num_classes * 4]),  # (B*head_B, num_classes*4)
            anchors=tf.reshape(tf.gather(tiled_anchors, self.head_kept_indices, batch_dims=1), (-1, 4)),
            image_width=self.image_width,
            image_height=self.image_height)  # (B*head_B, 4)

        # --------- Losses.
        (rpn_score_loss, rpn_bbox_loss,
         rpn_total_loss) = self.rpn.get_loss(
            pos_neg_scores=tf.reshape(self.rpn_pos_neg_scores, [-1]),
            pos_neg_weights=tf.constant(config.RPN_NEG_POS_WEIGHTS),
            scores_prob=tf.reshape(rpn_scores_prob, (-1, 2))[:, 1],  # (B*rpn_B,)
            bbox_targets=tf.reshape(self.rpn_bbox_targets, (-1, 4)),
            bbox_enc=tf.reshape(self.rpn_bbox_enc, (-1, 4)),
            loss_weights=tf.constant(config.RPN_LOSS_WEIGHTS),
            label_smoothing=config.RPN_LABEL_SMOOTHING,
            smooth_factor=tf.constant(config.SMOOTH_FACTOR, tf.float32))

        (head_cls_loss, head_bbox_loss,
         head_total_loss) = self.bbox_head.get_loss(
            bbox_targets_pred=self.head_bbox_enc,
            bbox_targets=tf.reshape(self.head_bbox_targets, [-1, self.num_classes * 4]),
            cls_logits=self.head_cls_scores,  # (B*head_B, num_classes)
            gt_cls=tf.reshape(self.head_gt_classes, [-1]),  # (B*head_B,)
            bg_obj_indices=tf.reshape(self.bg_obj_indices, [-1]),
            bg_obj_weights=tf.constant(config.BG_OBJ_WEIGHTS),
            loss_weights=tf.constant(config.HEAD_LOSS_WEIGHTS),
            label_smoothing=config.HEAD_LABEL_SMOOTHING,
            smooth_factor=tf.constant(config.SMOOTH_FACTOR, tf.float32),
            num_classes=self.num_classes,
            train_cls=config.TRAIN_HEAD_CLS,
            train_reg=config.TRAIN_HEAD_REG)

        pooled_masks_gt, pooled_masks_prob = self.bbox_masking.get_pooled_masks(
            roi_pooled_masks_probs=self.rois_pooled_masks_logits,
            pred_img_bboxes=tf.reshape(head_proposals, (-1, 4)),
            roi_gt_masks=tf.reshape(self.head_gt_masks, (-1, self.image_width, self.image_height)),
            gt_cls=tf.reshape(self.head_gt_classes, [-1]),
            bg_obj_indices=tf.reshape(self.bg_obj_indices, [-1]))

        mask_loss, mask_total_loss = self.bbox_masking.get_loss(
            y_true=tf.reshape(tf.cast(pooled_masks_gt, tf.float32), [-1]),
            y_prob=tf.reshape(pooled_masks_prob, [-1]),
            loss_weight=tf.constant(config.MASK_LOSS_WEIGHT))

        backbone_rgl_loss = tf.reduce_sum(self.backbone_model.model.losses)  # backbone regularizer loss
        total_loss = tf.reduce_sum([rpn_total_loss, mask_total_loss, head_total_loss]) + backbone_rgl_loss
        loss_vector = tf.stack([rpn_score_loss, rpn_bbox_loss, head_cls_loss,
                                head_bbox_loss, mask_loss])

        # ---------- Metrics.
        # Get precision, recall and IoU metrics.
        rpn_metrics = self.rpn.get_metrics(
            y_true=tf.reshape(tf.cast(self.rpn_pos_neg_scores, tf.float32), [-1]),  # (B*rpn_B,)
            y_pred=tf.reshape(rpn_scores_prob, (-1, 2))[:, 1],  # (B*rpn_B,), positives
            gt_bbox=pos_rpn_gt_bbox,
            proposals=pos_rpn_proposals)

        head_cls_metrics = self.bbox_head.get_class_metrics(
            y_true=tf.reshape(self.head_gt_classes, [-1]),
            y_pred=self.head_cls)

        # Get IoU and mean average precision (mAP) metrics.
        bg_obj_indices_vec = tf.reshape(self.bg_obj_indices, [-1])  # (B*nms,)
        gt_boxes_assigned = tf.gather(gt_boxes, self.gt_anchor_assignment, batch_dims=1)  # (B, n, 4)
        gt_boxes_assigned = tf.gather(gt_boxes_assigned, self.head_kept_indices, batch_dims=1)  # (B, nms, 4)
        pos_head_gt_bbox = tf.reshape(gt_boxes_assigned, [-1, 4])  # (B*nms, 4)
        head_bbox_metrics = self.bbox_head.get_box_metrics(
            gt_bbox=tf.cast(pos_head_gt_bbox, tf.float32),
            gt_cls=tf.reshape(self.head_gt_classes, [-1]),
            proposals=head_proposals,
            iou_thresh=tf.constant(config.HEAD_FG_THRESH),
            cls_prob=self.head_cls_prob,
            bg_obj_indices=bg_obj_indices_vec)

        mask_metrics = self.bbox_masking.get_metrics(
            y_true=tf.reshape(tf.cast(pooled_masks_gt, tf.float32), [-1]),
            y_prob=tf.reshape(pooled_masks_prob, [-1]))

        # Return metrics as lists
        metrics_ = rpn_metrics + head_cls_metrics + head_bbox_metrics + mask_metrics
        metric_values = [metric.result() for metric in metrics_]
        metric_names = [metric.name for metric in metrics_]

        # Select positive RPN proposals.
        rpn_bbox_in_1st_img = self.rpn_proposals[0]  # (rpn_B, 4)
        rpn_scores_in_1st_img = self.rpn_pos_neg_scores[0]  # (rpn_B,)

        # Select positive head boxes.
        head_bbox_in_1st_img = head_proposals[:config.HEAD_BATCH_SIZE]  # (head_B, 4)
        head_scores_in_1st_img = bg_obj_indices_vec[:config.HEAD_BATCH_SIZE]  # (head_B,)

        # Boxes to show in summary.
        summary_boxes = [(rpn_bbox_in_1st_img, rpn_scores_in_1st_img, 'rpn_pos_bbox'),
                         (head_bbox_in_1st_img, head_scores_in_1st_img, 'head_pos_bbox')]

        return loss_vector, total_loss, summary_boxes, metric_values, metric_names

    def run_networks(self, inputs, gt_boxes=None, gt_cls=None, gt_masks=None, is_training=True):
        """Runs the backbone, RPN and HEAD networks to estimate
        box coordinates and related labels.

        Args:
            inputs (tensor): the input tensors to the learning model.
            gt_boxes (tensor): ground truth box coordinates to learn from.
            gt_cls (tensor): reference data to learn from (labels).
            gt_masks (tensor): reference masks from generated data..
            is_training (bool): if True, the backbone model is trainable.
        """
        # Get last feature maps of backbone model.
        backbone_output = self.backbone_model(inputs, is_training)

        # Get dimension parameters.
        backbone_dim = tf.shape(backbone_output)[1]
        stride = tf.cast(self.image_width // backbone_dim, tf.float32)

        # Iterate through feature layer outputs.
        pos_neg_scores, gt_bbox_assignment, gt_bbox_assignment_ratio = [], [], []

        # Generate anchors corner coordinates.
        self.anchors, _ = anchor_generate.get_anchors_corners(
            image_width=tf.cast(self.image_width, tf.float32),
            image_height=tf.cast(self.image_height, tf.float32),
            anchor_scales=tf.cast(tf.constant(config.ANCHOR_SCALES), tf.float32),
            anchor_ratios=tf.constant(config.ANCHOR_RATIOS),
            stride=stride)  # (W * H * 9, 4)

        # Get region proposals.
        (self.rpn_anchor_proposals, self.rpn_anchor_scores_prob,
         self.rpn_anchor_scores, self.rpn_anchor_bbox) = self.rpn(
            inputs=backbone_output, anchors=self.anchors)  # proposals: (B, num_anchors, 4)

        # Get batched anchor labels (50% positive and neg.). Note
        # that every position in the feature map has 9 anchors, and
        # every anchor has two possible labels (background and
        # foreground). If we make the depth of the feature map as 18
        # (9 anchors x 2 labels), we will make every anchor have a
        # vector with two values representing foreground and background.
        # The get_labels method filters out anchors not matching ground
        # truth boxes by IOU (assign -1).
        pos_neg_anchors = tf.TensorArray(tf.float32, size=self.batch_size)
        class_assignment = tf.TensorArray(tf.int32, size=self.batch_size)
        assignment_ratio = tf.TensorArray(tf.float32, size=self.batch_size)
        for k in tf.range(self.batch_size):
            _scores, _assignments, _assignment_ratio = anchor_labels.get_labels(
                gt_boxes=tf.cast(gt_boxes[k], tf.float32),
                anchors=self.anchors,
                anchor_batch_size=tf.constant(config.RPN_BATCH_SIZE, dtype=tf.int32),
                overlaps_pos=tf.constant(config.RPN_ANCHOR_FG_THRESH),
                overlaps_neg=tf.constant(config.RPN_ANCHOR_BG_THRESH),
                positive_ratio=tf.constant(config.RPN_FG_RATIO),
                image_width=self.image_width,
                image_height=self.image_height)  # (W * H * 9, 1)
            # Add up current item to list.
            pos_neg_anchors = pos_neg_anchors.write(k, _scores)
            class_assignment = class_assignment.write(k, _assignments)
            assignment_ratio = assignment_ratio.write(k, _assignment_ratio)

        pos_neg_scores.append(pos_neg_anchors.stack())  # F*(B, num_anchors)
        gt_bbox_assignment.append(class_assignment.stack())  # F*(B, num_anchors)
        gt_bbox_assignment_ratio.append(assignment_ratio.stack())  # F*(B,)

        # Concatenate anchors and proposals.
        self.anchor_pos_neg_scores = tf.concat(pos_neg_scores, axis=-1)  # (B, F*num_anchors)
        self.gt_anchor_assignment = tf.concat(gt_bbox_assignment, axis=-1)  # (B, F*num_anchors)
        self.rpn_assignment_ratio = tf.concat(gt_bbox_assignment_ratio, axis=0)  # (B,)

        # Apply non-maximum suppression (NMS) to filter out
        # non-matching anchors (negatives) and the matching
        # ones (positives), given IoU values. <rois_coord> is
        # (num_positives, 5), with 1st column representing the
        # image index.
        if is_training:
            top_n = config.MAX_RPN_POST_NMS_TOP_N
        else:
            top_n = config.TEST_MAX_RPN_POST_NMS_TOP_N

        self.kept_indices = tf.map_fn(
            fn=lambda x: nms.apply_nms(
                bboxes=x[0],
                scores_prob=x[1],
                top_n=top_n,
                iou_thresh=config.NMS_THRESH,
                image_width=tf.cast(self.image_width, tf.float32),
                image_height=tf.cast(self.image_height, tf.float32),
                sigma=tf.constant(config.NMS_SIGMA)),
            elems=[self.rpn_anchor_proposals, self.rpn_anchor_scores_prob[:, :, 1]],
            fn_output_signature=tf.int32)  # (B, nms)

        # Collect boxes and scores given indices.
        self.nms_bbox = tf.cast(tf.gather(
            self.rpn_anchor_proposals, self.kept_indices, batch_dims=1), tf.float32)  # (B, nms, 4)
        self.nms_scores = tf.gather(
            self.rpn_anchor_scores_prob[:, :, 1], self.kept_indices, batch_dims=1)  # (B, nms, 1)

        # Convert positives as background/foreground given the IOU,
        # then compute respective target coordinates to learn, i.e.
        # (tx, ty, tw, th) per class and per batch item.
        head_bbox = tf.TensorArray(tf.float32, size=self.batch_size)
        head_scores = tf.TensorArray(tf.float32, size=self.batch_size)
        head_gt_classes = tf.TensorArray(tf.int32, size=self.batch_size)
        head_gt_masks = tf.TensorArray(tf.int32, size=self.batch_size)
        bbox_targets = tf.TensorArray(tf.float32, size=self.batch_size)
        bbox_weights = tf.TensorArray(tf.float32, size=self.batch_size)
        bg_obj_indices = tf.TensorArray(tf.float32, size=self.batch_size)
        keep_indices = tf.TensorArray(tf.int64, size=self.batch_size)
        assigned_bbox_ratio = tf.TensorArray(tf.float32, size=self.batch_size)
        head_ious = tf.TensorArray(tf.float32, size=self.batch_size)
        for b in tf.range(self.batch_size):
            (_bbox, _gt_classes, roi_scores_, _gt_masks,
             _bbox_targets, _bbox_weights,
             _bg_obj_indices, _keep_indices,
             _assigned, _ious) = anchor_targets.build_targets(
                roi_bbox=self.nms_bbox[b],  # (nms, 4)
                roi_scores=self.nms_scores[b],
                gt_bbox=tf.cast(gt_boxes[b], tf.float32),
                gt_cls=gt_cls[b],
                gt_masks=gt_masks,
                foreground_rate=tf.constant(config.HEAD_FG_RATE),
                foreground_thresh=tf.constant(config.HEAD_FG_THRESH),
                background_thresh=tf.constant(config.HEAD_BG_THRESH_LO),
                normalize_targets=config.BBOX_NORMALIZE_TARGETS,
                norm_mean_bbox=tf.constant(config.BBOX_NORM_MEAN),
                norm_std_bbox=tf.constant(config.BBOX_NORM_STD),
                batch_size=tf.constant(config.HEAD_BATCH_SIZE),
                num_classes=self.num_classes,
                resample_positives=config.HEAD_RESAMPLE_POSITIVES
            )

            # Add up current item to list.
            head_bbox = head_bbox.write(b, _bbox)
            head_scores = head_scores.write(b, roi_scores_)
            head_gt_classes = head_gt_classes.write(b, _gt_classes)
            head_gt_masks = head_gt_masks.write(b, _gt_masks)
            bbox_targets = bbox_targets.write(b, _bbox_targets)
            bbox_weights = bbox_weights.write(b, _bbox_weights)
            bg_obj_indices = bg_obj_indices.write(b, _bg_obj_indices)
            keep_indices = keep_indices.write(b, _keep_indices)
            assigned_bbox_ratio = assigned_bbox_ratio.write(b, _assigned)
            head_ious = head_ious.write(b, _ious)

        # Stack items.
        self.head_bbox_from_rpn = head_bbox.stack()  # (B, head_B, 4)
        self.head_scores_from_rpn = head_scores.stack()  # (B, head_B, 1)
        self.head_gt_classes = head_gt_classes.stack()
        self.head_gt_masks = head_gt_masks.stack()
        self.head_bbox_targets = bbox_targets.stack()
        self.head_bbox_weights = bbox_weights.stack()
        self.bg_obj_indices = bg_obj_indices.stack()
        self.head_assignment_ratio = assigned_bbox_ratio.stack()  # (B, head_B)
        self.head_ious = head_ious.stack()
        keep_indices = keep_indices.stack()

        # Collect head indices.
        self.head_kept_indices = tf.gather(self.kept_indices, keep_indices, batch_dims=1)  # (B, nms)

        scores_targets = tf.TensorArray(tf.int32, size=self.batch_size)
        scores_enc = tf.TensorArray(tf.float32, size=self.batch_size)
        bbox_targets = tf.TensorArray(tf.float32, size=self.batch_size)
        bbox_enc = tf.TensorArray(tf.float32, size=self.batch_size)
        proposals = tf.TensorArray(tf.float32, size=self.batch_size)
        gt_assignments = tf.TensorArray(tf.int32, size=self.batch_size)
        for m in tf.range(self.batch_size):
            (_scores_gt, _scores_enc,
             _bbox_targets, _bbox_enc,
             _proposals, _gt_assign) = self.rpn.build_rpn_targets(
                anchors=self.anchors,
                gt_index_assignments=self.gt_anchor_assignment[m],
                gt_boxes=gt_boxes[m],
                anchor_scores=self.anchor_pos_neg_scores[m],
                rpn_scores=self.rpn_anchor_scores[m],  # (B, F*rpn_B, 2)
                rpn_bbox_enc=self.rpn_anchor_bbox[m],  # (B, F*rpn_B, 4)
                rpn_proposals=self.rpn_anchor_proposals[m],  # (B, F*rpn_B, 4)
                normalize=config.BBOX_NORMALIZE_TARGETS,
                norm_mean=tf.constant(config.BBOX_NORM_MEAN),
                norm_std=tf.constant(config.BBOX_NORM_STD),
                batch_size=tf.cast(config.RPN_BATCH_SIZE, tf.float32),
                resample_positives=config.RPN_RESAMPLE_POSITIVES,
                resample_rate=tf.constant(config.RPN_RESAMPLE_RATE))

            # Add up current item to list.
            scores_targets = scores_targets.write(m, _scores_gt)
            scores_enc = scores_enc.write(m, _scores_enc)
            bbox_targets = bbox_targets.write(m, _bbox_targets)
            bbox_enc = bbox_enc.write(m, _bbox_enc)
            proposals = proposals.write(m, _proposals)
            gt_assignments = gt_assignments.write(m, _gt_assign)

        self.rpn_pos_neg_scores = scores_targets.stack()  # (B, F*rpn_B)
        self.rpn_scores_enc = scores_enc.stack()
        self.rpn_bbox_targets = bbox_targets.stack()  # (B, F*rpn_B, 4)
        self.rpn_bbox_enc = bbox_enc.stack()
        self.rpn_proposals = proposals.stack()  # (B, F*rpn_B, 4)
        self.rpn_gt_assignments = gt_assignments.stack()  # (B, F*rpn_B)

        # Apply RoI align.
        batch_ids = tf.tile(
            tf.reshape(tf.range(self.batch_size), shape=(-1, 1)),
            multiples=[1, config.HEAD_BATCH_SIZE])  # (B, head_B)

        rois_pooled = tf.map_fn(
            fn=lambda x: roi_ops.roi_align(
                feature_maps=backbone_output,  # (nF, B, P, P, F)
                boxes=x[0],
                box_indices=x[1],
                pre_pool_size=config.HEAD_PRE_POOL_SIZE,
                image_size=[self.image_width, self.image_height]),
            elems=[self.head_bbox_from_rpn, batch_ids],
            fn_output_signature=tf.float32)  # (B, rpn_B, P*2, P*2, F)

        # Get class and bounding box predictions.
        channel_dim = backbone_output[0].get_shape()[-1]
        out_shape = (-1, config.HEAD_PRE_POOL_SIZE * 2, config.HEAD_PRE_POOL_SIZE * 2, channel_dim)
        rois_pooled = tf.reshape(rois_pooled, shape=out_shape, name="rois_pooled")

        (self.head_cls_scores, self.head_cls_prob,
         self.head_cls, self.head_bbox_enc) = self.bbox_head(
            inputs=rois_pooled)

        # --------------- MASKING
        rois_pooled_mask = tf.map_fn(
            fn=lambda x: roi_ops.roi_align(
                feature_maps=backbone_output,
                boxes=x[0],
                box_indices=x[1],
                pre_pool_size=config.HEAD_PRE_POOL_SIZE,
                image_size=[self.image_width, self.image_height]),
            elems=[self.head_bbox_from_rpn, batch_ids],
            fn_output_signature=tf.float32)  # (B, num_boxes, P*2, P*2, num_feat*F) / P*2=pre-pool**crop_size

        # (module) Contextual fusion.
        # Use one box with image size.
        img_boxes = tf.constant([[0, 0, self.image_width - 1, self.image_height - 1]])  # (1, 4)
        img_boxes = tf.expand_dims(tf.cast(img_boxes, tf.float32), axis=0)  # (1, 1, 4)
        img_boxes = tf.tile(img_boxes, multiples=[self.batch_size, 1, 1])  # (B, 1, 4)
        tmp_ids = tf.reshape(tf.range(self.batch_size), shape=(-1, 1))  # (B, 1)

        # Get pooled feature from the highest
        # resolution feature map.
        img_pooled = tf.map_fn(
            fn=lambda x: roi_ops.roi_align(
                feature_maps=[backbone_output[0]],
                boxes=x[0],
                box_indices=x[1],
                pre_pool_size=config.HEAD_PRE_POOL_SIZE,
                image_size=[self.image_width, self.image_height]),
            elems=[img_boxes, tmp_ids],
            fn_output_signature=tf.float32)
        img_pooled = tf.squeeze(img_pooled, axis=1)  # (B, P*2, P*2, F)

        # Assert shape of pooled features (initially unknown due to transpose).
        shp = config.HEAD_PRE_POOL_SIZE * 2
        channel_dim = self.backbone_model.last_conv_channel_dim
        img_pooled = tf.ensure_shape(img_pooled, (self.batch_size, shp, shp, channel_dim))

        if not config.TRAIN_MASK:
            img_pooled = tf.stop_gradient(img_pooled)

        # Add it to pooled RoIs.
        rois_pooled_mask = tf.math.add(rois_pooled_mask, tf.expand_dims(img_pooled, axis=1))
        rois_pooled_mask = tf.reshape(rois_pooled_mask, shape=out_shape)  # (B, P*2, P*2, F)

        # Get mask predictions. Note: they are soft masks,
        # represented by float numbers, so they hold more
        # details than binary masks.
        self.rois_pooled_masks_prob, self.rois_pooled_masks_logits = self.bbox_masking(
            inputs=rois_pooled_mask)  # (B, 28, 28, num_classes)

    def save_variables(self, directory, checkpoint):
        """Saves weights of variables from the learning model."""
        # Save layer weights from sub-models separately.
        for net in self.networks:
            filename = net.name
            filepath = os.path.join(directory, filename)
            net.save_weights(filepath)

        # Save learning rate.
        var_file_path = os.path.join(directory, "variables")
        checkpoint.write(var_file_path)

    def load_variables(self, directory, checkpoint,
                       load_learning_rate=True, class_names=None,
                       load_backbone=config.LOAD_BACKBONE_WEIGHTS,
                       load_rpn=config.LOAD_RPN_WEIGHTS,
                       load_head=config.LOAD_HEAD_WEIGHTS,
                       load_mask=config.LOAD_MASK_WEIGHTS,
                       load_checkpoint=config.LOAD_CHECKPOINT):
        """Loads weights of variables from the learning model.

        Do not load Keras model. Note that it is best
        practice to load weights (not a model topology) for
        custom (subclassed) models due to Python
        custom codes that cannot be serialized safely
        (i.e. you need access to the code that builds up the
        model, compared to Sequential/Functional models
        which are tensorflow datastructures).
        """
        if load_backbone:
            filename = self.backbone_model.model.name
            filepath = os.path.join(directory, filename)

            # Weights can be loaded if same model structure
            # and custom codes.
            self.backbone_model.model.load_weights(filepath).expect_partial()
            print(f"'{filename}' weights loaded.")

        if load_rpn:
            filename = self.rpn.name
            filepath = os.path.join(directory, filename)

            # Call network once to access its variables
            # before restoring.
            self.rpn.build_variables_from_call(
                last_dimension=self.backbone_model.last_conv_channel_dim)

            # Weights can be loaded if same model structure
            # and custom codes.
            self.rpn.load_weights(filepath).expect_partial()
            print(f"'{filename}' weights loaded.")

        if load_head:
            filename = self.bbox_head.name
            filepath = os.path.join(directory, filename)

            # Call network once to access its variables
            # before restoring.
            self.bbox_head.build_variables_from_call(
                last_dim_of_input=self.backbone_model.last_conv_channel_dim,
                pool_size=config.HEAD_PRE_POOL_SIZE * 2)

            # Weights can be loaded if same model structure
            # and custom codes.
            self.bbox_head.load_weights(filepath).expect_partial()
            print(f"'{filename}' weights loaded.")

        if load_mask:
            filename = self.bbox_masking.name
            filepath = os.path.join(directory, filename)

            # Call network once to access its variables
            # before restoring.
            self.bbox_masking.build_variables_from_call(
                last_dim_of_input=self.backbone_model.last_conv_channel_dim)

            # Weights can be loaded if same model structure
            # and custom codes.
            self.bbox_masking.load_weights(filepath).expect_partial()
            print(f"'{filename}' weights loaded.")

        # Load checkpoint.
        if load_checkpoint:
            lr_file_path = os.path.join(directory, "variables")
            checkpoint.restore(lr_file_path).expect_partial()

            # Load class names and values.
            try:
                # Convert variables to tensors which can
                # can be used for iterations.
                self.class_names.assign(checkpoint.class_names)
                self.class_values.assign(checkpoint.class_values)
                if not all([item in tf.convert_to_tensor(self.class_names) for item in class_names]):
                    raise Exception("\nNames of classes in config do not correspond to those restored in model."
                                    f"\n\tprovided by user: '{class_names}'"
                                    f"\n\trestored by model: '{self.class_names}'")

            except AttributeError:
                warning = "WARNING: No class names loaded from checkpoint. "
                warning += "Names from generator loaded instead."
                tf.print(warning)
                self.class_names = tf.Variable([], dtype=tf.string, trainable=False, shape=tf.TensorShape(None))
                self.class_values = tf.Variable([], dtype=tf.int32, trainable=False, shape=tf.TensorShape(None))

        # Load learning rate from checkpoint.
        if load_learning_rate:
            self.learning_rate.assign(checkpoint.learning_rate)
            print("'learning_rate' loaded from checkpoint.")

    @tf.function
    def optimize_step(self, model_args):
        """Returns the losses and metrics given input tensors,
        and optimize the detector internal model networks.

         Args:
             model_args (list): arguments to the call method.
         """
        with tf.GradientTape() as tape:
            losses, total_loss, boxes_prop, metrics_, metric_names = self.call(*model_args)
            # Get gradients from variables.
            grads = tape.gradient(total_loss, self.trainable_variables)
            # Update parameters.
            self.optimizer.apply_gradients(zip(grads, self.trainable_variables))

        return losses, total_loss, boxes_prop, metrics_, metric_names

    @tf.function(input_signature=(
            [tf.TensorSpec(shape=(None, None, None, 3), dtype=tf.float32),
             tf.TensorSpec(shape=(None, None, 4), dtype=tf.int32),
             tf.TensorSpec(shape=(None, None), dtype=tf.int32),
             tf.TensorSpec(shape=(None, None, None, None), dtype=tf.int32)],
            tf.TensorSpec(shape=None, dtype=tf.bool)))
    def train_step(self, model_args, optimize):
        """Returns the losses and metrics given input tensors,
        and optimize the detector internal model networks.

         Args:
             model_args (list): arguments to the call method.
             optimize (bool): if True, optimize the model networks.
         """
        if optimize:
            losses, total_loss, boxes_prop, metrics_, metric_names = self.optimize_step(model_args)
            # Define loss vector and add to average calculation.
            return tf.concat([[total_loss], losses], axis=0), boxes_prop, metrics_, metric_names
        else:
            # Define loss vector and add to average calculation.
            losses, total_loss, boxes_prop, metrics_, metric_names = self.call(*model_args)
            return tf.concat([[total_loss], losses], axis=0), boxes_prop, metrics_, metric_names

    @staticmethod
    def get_weight_vector(index, num_classes):
        """Returns the hot-one encoding of box coordinates, given
        the labels. For example, if there exist 4 classes, the
        coordinate matrix will be of shape (B, 4*num_classes). As
        such, if the first instance of the batch is assigned to
        label 2, then related one-hot vector will be eight 0s,
        followed by four 1s, followed by four 0s."""
        row_size = 4 * num_classes
        left = index[0] * 4
        right = row_size - (left + 4)
        return tf.pad(tf.cast(tf.ones((4,)), tf.int32), [[left, right]])

    @staticmethod
    def select_from_first_and_last_axis(indices, inputs_to_select):
        row_index = tf.cast(indices[0], tf.int32)
        last_index_select = tf.cast(indices[1], tf.int32)
        return inputs_to_select[row_index, ..., last_index_select]

    @tf.function
    def predict(self, inputs, nms_top_n, nms_iou_thresh, obj_score_thresh=0.,
                soft_nms_sigma=0., predict_from_rpn=False):
        """Returns the class, object scores and bounding box proposals
        predicted from inputs.

        Args:
            inputs (tensor): the image inputs extracted from inputs via
                bounding boxes.
            nms_top_n (int): maximum number of objects to detect in a
                img. This parameter is relevant for non-maximum
                suppression in learning model.
            nms_iou_thresh (float): a threshold value above which two
                prediction boxes are considered duplicates if their
                overlap ratio exceeds the value. This parameter is
                relevant when calculating the intersection over union
                (iou).
            obj_score_thresh (float): object score above which associated
                boxes estimated are returned.
            soft_nms_sigma (float): soft-NMS parameter. Deactivated
                if set to 0. if not 0, NMS reduces the score of other
                overlapping boxes instead of directly causing them to
                be pruned. Consequently, it returns the new scores of
                each input box in the second output.
                Read: https://arxiv.org/abs/1704.04503 for info.
            predict_from_rpn (bool): if True, the argument nms_top_n
                is considered (as replacement the one pre-configured).
        """
        # Get last feature maps of backbone model.
        input_dim = tf.shape(inputs)[1]
        backbone_output = self.backbone_model(inputs, is_training=False)  # (B,W,H,C)

        # Get dimension parameters.
        backbone_dim = tf.shape(backbone_output)[1]
        stride = tf.cast(input_dim // backbone_dim, tf.float32)

        # Generate anchors corner coordinates.
        anchors, _ = anchor_generate.get_anchors_corners(
            image_width=tf.cast(input_dim, tf.float32),
            image_height=tf.cast(input_dim, tf.float32),
            anchor_scales=tf.cast(tf.constant(config.ANCHOR_SCALES), tf.float32),
            anchor_ratios=tf.constant(config.ANCHOR_RATIOS),
            stride=stride)  # (W * H * 9, 4)

        # Get region proposals.
        rpn_proposals, rpn_scores_prob, _, _ = self.rpn(
            inputs=backbone_output,
            anchors=anchors)  # proposals: (num_anchors, 4)

        rpn_scores_prob = tf.reshape(rpn_scores_prob, (-1, 2))
        rpn_proposals = tf.reshape(rpn_proposals, (-1, 4))  # (B*n, 4)

        # Apply NMS.
        top_n = config.TEST_MAX_RPN_POST_NMS_TOP_N if not predict_from_rpn else nms_top_n
        nms_indices = nms.apply_nms(
            bboxes=rpn_proposals,
            scores_prob=rpn_scores_prob[:, 1],
            top_n=top_n,
            iou_thresh=nms_iou_thresh,
            image_width=tf.cast(input_dim, tf.float32),
            image_height=tf.cast(input_dim, tf.float32),
            sigma=tf.cast(config.NMS_SIGMA, tf.float32))
        nms_bbox = tf.gather(tf.cast(rpn_proposals, tf.float32), nms_indices)  # (nms, 4)
        nms_scores = tf.gather(rpn_scores_prob[:, 1], nms_indices)  # (nms, 1)
        nms_anchors = tf.gather(anchors, nms_indices)  # (nms, 4)

        # Apply RoI pooling.
        batch_ids = tf.zeros((tf.shape(nms_bbox)[0]), dtype=tf.int32)  # (nms,)
        self.rois_pooled = roi_ops.roi_align(
            feature_maps=backbone_output,
            boxes=nms_bbox,
            box_indices=batch_ids,
            pre_pool_size=config.HEAD_PRE_POOL_SIZE,
            image_size=[input_dim, input_dim])  # (nms, pool_size, pool_size, F)

        shp = config.HEAD_PRE_POOL_SIZE * 2
        channel_dim = self.backbone_model.last_conv_channel_dim
        rois_pooled = tf.ensure_shape(
            self.rois_pooled,
            shape=(top_n, shp, shp, channel_dim),
            name="rois_pooled")

        # Get predictions.
        (head_cls_scores, head_cls_prob,
         head_cls, head_bbox_enc) = self.bbox_head(rois_pooled)

        # Select relevant boxes given class.
        cls_indices = tf.reshape(head_cls, (-1, 1))
        weight_matrix = tf.map_fn(
            fn=lambda x: self.get_weight_vector(
                index=x,
                num_classes=self.num_classes),
            elems=tf.cast(cls_indices, tf.int32))  # (nms, 4 * num_classes)
        selected_bbox_targets_pred = tf.boolean_mask(head_bbox_enc, tf.greater(weight_matrix, 0))
        head_bbox_targets = tf.reshape(selected_bbox_targets_pred, (-1, 4))  # (nms, 4)

        # Convert box predictions into corner coordinates.
        head_proposals = bbox_ops.get_head_proposals_from_targets(
            anchors=nms_anchors,
            targets=head_bbox_targets,
            image_width=tf.cast(input_dim, tf.float32),
            image_height=tf.cast(input_dim, tf.float32))  # (nms, 4)

        head_indices = nms.apply_nms(
            bboxes=head_proposals,
            scores_prob=nms_scores,  # (nms,)
            top_n=nms_top_n,
            iou_thresh=nms_iou_thresh,
            image_width=tf.cast(input_dim, tf.float32),
            image_height=tf.cast(input_dim, tf.float32),
            sigma=soft_nms_sigma  # set to 0 if you want to prune boxes (avoid multi-boxes at same location)
        )
        head_proposals = tf.gather(tf.cast(head_proposals, tf.float32), head_indices)  # (nms, 4)
        head_cls = tf.gather(head_cls, head_indices)
        head_cls_prob = tf.gather(head_cls_prob, head_indices)  # (nms, num_classes)
        head_obj_scores = tf.gather(nms_scores, head_indices)  # (nms, 1)

        # Keep head boxes whose object score is at given threshold.
        obj_indices = tf.reshape(tf.where(head_obj_scores >= obj_score_thresh), [-1])  # (obj,)
        head_proposals = tf.gather(head_proposals, obj_indices)  # (obj, 4)
        head_cls = tf.gather(head_cls, obj_indices)
        head_cls_prob = tf.gather(head_cls_prob, obj_indices)  # (obj, num_classes)
        head_obj_scores = tf.gather(head_obj_scores, obj_indices)

        # ---------------------------- MASK
        # Select masks given indices from head NMS.
        nms_head_bbox = tf.gather(nms_bbox, head_indices)
        batch_ids = tf.zeros((tf.shape(nms_head_bbox)[0]), dtype=tf.int32)  # (nms,)
        self.rois_pooled_mask = roi_ops.roi_align(
            feature_maps=backbone_output,
            boxes=nms_head_bbox,
            box_indices=batch_ids,
            pre_pool_size=config.HEAD_PRE_POOL_SIZE,
            image_size=[input_dim, input_dim])  # (num_boxes, P*2, P*2, num_feat*F)

        # (module) Contextual fusion.
        # Use one box with image size.
        img_boxes = tf.constant([[0, 0, self.image_width - 1, self.image_height - 1]])  # (1, 4)

        # Get pooled feature from the highest
        # resolution feature map.
        img_pooled = roi_ops.roi_align(
            feature_maps=[backbone_output[0]],
            boxes=tf.cast(img_boxes, tf.float32),
            box_indices=tf.range(1),
            pre_pool_size=config.HEAD_PRE_POOL_SIZE,
            image_size=[self.image_width, self.image_height])  # (1, P*2, P*2, F)

        # Assert shape of pooled features (initially unknown due to transpose).
        shp = config.HEAD_PRE_POOL_SIZE * 2
        channel_dim = self.backbone_model.last_conv_channel_dim
        img_pooled = tf.ensure_shape(img_pooled, (1, shp, shp, channel_dim))

        # Add it to pooled RoIs.
        self.rois_pooled_mask = tf.math.add(self.rois_pooled_mask, img_pooled)
        pooled_masks_prob, _ = self.bbox_masking(
            inputs=self.rois_pooled_mask)  # (B, 28, 28, num_classes)

        # Select relevant masks given class.
        row_indices = tf.reshape(tf.range(tf.shape(head_cls)[0]), (-1, 1))
        indices = tf.concat([row_indices, tf.cast(tf.reshape(head_cls, (-1, 1)), tf.int32)], axis=-1)  # (B, 2)
        best_masks = tf.map_fn(
            fn=lambda x: self.select_from_first_and_last_axis(x, pooled_masks_prob),
            elems=tf.cast(indices, tf.float32))  # (B, 28, 28)

        return (head_cls, tf.reduce_max(head_cls_prob, axis=-1),
                head_proposals, head_obj_scores,
                nms_bbox, tf.reshape(nms_scores, (-1,)), best_masks)
